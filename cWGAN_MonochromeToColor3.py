# -*- coding: utf-8 -*-
"""MonochromeToColor3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tk9b5yqQkyvS8vPRqGql3UCUyd2pRzm1
"""

# import kagglehub

# # Download latest version
# path = kagglehub.dataset_download("shravankumar9892/image-colorization")

# print("Path to dataset files:", path)

# !pip install torchsummary

# !pip install pytorch-lightning

# Commented out IPython magic to ensure Python compatibility.
import os
from pathlib import Path

# Import glob to get the files directories recursively
import glob

# Import Garbage collector interface
import gc

# Import OpenCV to transforme pictures
import cv2

# Import Time
import time

# import numpy for math calculations
import numpy as np

# Import pandas for data (csv) manipulation
import pandas as pd

# Import matplotlib for plotting
import matplotlib.pyplot as plt
import matplotlib
matplotlib.style.use('fivethirtyeight')
# %matplotlib inline

import PIL
from PIL import Image
from skimage.color import rgb2lab, lab2rgb

import pytorch_lightning as pl

# Import pytorch to build Deel Learling Models
import torch
from torch import nn, optim
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
from torchvision import models
from torch.nn import functional as F
import torch.utils.data
from torchvision.models.inception import inception_v3
from scipy.stats import entropy

# learning rate scheduler
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau

from torchsummary import summary

# Import tqdm to show a smart progress meter
from tqdm import tqdm

# Import warnings to hide the unnessairy warniings
import warnings
warnings.filterwarnings('ignore')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# npy 파일 열기
gray_scale_path = "data/l/gray_scale.npy"
ab1_path = "data/ab/ab1.npy"
ab2_path = "data/ab/ab2.npy"

# 데이터를 로드
L_df = (np.load(gray_scale_path))[:20000]
ab1_df = (np.load(ab1_path))
ab2_df = (np.load(ab2_path))

ab_df = np.concatenate((ab1_df, ab2_df), axis=0)

# 데이터 확인
print("Gray Scale Shape:", L_df.shape) # (h, w) 형식
print("AB Shape:", ab_df.shape) # (h, w, 2) 형식

print("AB min:", ab_df.min(), "max:", ab_df.max())

L_df.min(), L_df.max()

dataset = (L_df, ab_df)

def lab_to_rgb(L, ab):
    """
    Takes an image or a batch of images and converts from LAB space to RGB
    """
    L = L  * 100
    ab = (ab - 0.5) * 128 * 2
    Lab = torch.cat([L, ab], dim=2).numpy()
    rgb_imgs = []
    for img in Lab:
        img_rgb = lab2rgb(img)
        rgb_imgs.append(img_rgb)
    return np.stack(rgb_imgs, axis=0)

plt.figure(figsize=(30,30))
for i in range(1,16,2):
    plt.subplot(4,4,i)
    img = np.zeros((224,224,3))
    img[:,:,0] = L_df[i]
    plt.title('B&W')
    plt.imshow(lab2rgb(img))

    plt.subplot(4,4,i+1)
    img[:,:,1:] = ab1_df[i]
    img = img.astype('uint8')
    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)
    plt.title('Colored')
    plt.imshow(img)

plt.figure(figsize=(30,30))
for i in range(1,65,2):
    plt.subplot(8,8,i)
    img = np.zeros((224,224,3))
    img[:,:,0] = L_df[i]
    plt.title('B&W')
    plt.imshow(lab2rgb(img))

    plt.subplot(8,8,i+1)
    img[:,:,1:] = ab1_df[i]
    img = img.astype('uint8')
    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)
    plt.title('Colored')
    plt.imshow(img)

gc.collect()

"""## Data Loader"""

class ImageColorizationDataset(Dataset):
    ''' Black and White (L) Images and corresponding A&B Colors'''
    def __init__(self, dataset, transform=None):
        '''
        :param dataset: Dataset name.
        :param data_dir: Directory with all the images.
        :param transform: Optional transform to be applied on sample
        '''
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset[0])

    def __getitem__(self, idx):
        L = np.array(dataset[0][idx]).reshape((224,224,1))
        L = transforms.ToTensor()(L)

        ab = np.array(dataset[1][idx])
        ab = transforms.ToTensor()(ab)

        return ab, L

batch_size = 1

# Prepare the Datasets
train_dataset = ImageColorizationDataset(dataset = (L_df, ab_df))
test_dataset = ImageColorizationDataset(dataset = (L_df, ab_df))

# Build DataLoaders
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True, pin_memory = True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = False, pin_memory = True)

class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, stride=stride, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels,kernel_size=3,padding=1, stride=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        self.identity_map = nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride)
        self.relu = nn.ReLU(inplace=True)
    def forward(self, inputs):
        x = inputs.clone().detach()
        out = self.layer(x)
        residual  = self.identity_map(inputs)
        skip = out + residual
        return self.relu(skip)

class DownSampleConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.layer = nn.Sequential(
            nn.MaxPool2d(2),
            ResBlock(in_channels, out_channels)
        )

    def forward(self, inputs):
        return self.layer(inputs)

class UpSampleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.upsample = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=True)
        self.res_block = ResBlock(in_channels + out_channels, out_channels)

    def forward(self, inputs, skip):
        x = self.upsample(inputs)
        x = torch.cat([x, skip], dim=1)
        x = self.res_block(x)
        return x

class Generator(nn.Module):
    def __init__(self, input_channel, output_channel, dropout_rate = 0.2):
        super().__init__()
        self.encoding_layer1_ = ResBlock(input_channel,64)
        self.encoding_layer2_ = DownSampleConv(64, 128)
        self.encoding_layer3_ = DownSampleConv(128, 256)
        self.bridge = DownSampleConv(256, 512)
        self.decoding_layer3_ = UpSampleConv(512, 256)
        self.decoding_layer2_ = UpSampleConv(256, 128)
        self.decoding_layer1_ = UpSampleConv(128, 64)
        self.output = nn.Conv2d(64, output_channel, kernel_size=1)
        self.dropout = nn.Dropout2d(dropout_rate)

    def forward(self, inputs):
        ###################### Enocoder #########################
        e1 = self.encoding_layer1_(inputs)
        e1 = self.dropout(e1)
        e2 = self.encoding_layer2_(e1)
        e2 = self.dropout(e2)
        e3 = self.encoding_layer3_(e2)
        e3 = self.dropout(e3)

        ###################### Bridge #########################
        bridge = self.bridge(e3)
        bridge = self.dropout(bridge)

        ###################### Decoder #########################
        d3 = self.decoding_layer3_(bridge, e3)
        d2 = self.decoding_layer2_(d3, e2)
        d1 = self.decoding_layer1_(d2, e1)

        ###################### Output #########################
        output = self.output(d1)
        return output

model = Generator(1,2).to(device)
# summary(model, (1, 224, 224), batch_size = 1)

class Critic(nn.Module):
    def __init__(self, in_channels=3):
        super(Critic, self).__init__()

        def critic_block(in_filters, out_filters, normalization=True):
            """Returns layers of each critic block"""
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalization:
                layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *critic_block(in_channels, 64, normalization=False),
            *critic_block(64, 128),
            *critic_block(128, 256),
            *critic_block(256, 512),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(512, 1)
        )

    def forward(self, ab, l):
        # Concatenate image and condition image by channels to produce input
        img_input = torch.cat((ab, l), 1)
        output = self.model(img_input)
        return output

model = Critic(3).to(device)
# summary(model, [(2, 224, 224), (1, 224, 224)], batch_size = 1)

# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch
def _weights_init(m):
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
    if isinstance(m, nn.BatchNorm2d):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
        torch.nn.init.constant_(m.bias, 0)

def save_progress(cond, real, fake, current_epoch, output_dir="output3_images", figsize=(20,15)):
    """
    Save cond, real (original), and generated (fake) images in one panel as a single image.

    Parameters:
        cond: Tensor - Input condition image (L channel)
        real: Tensor - Ground truth image (AB channels)
        fake: Tensor - Generated image (AB channels)
        current_epoch: int - Current epoch number (used in filename)
        output_dir: str - Directory to save the images
        figsize: tuple - Figure size for saving
    """
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Detach tensors and move to CPU
    cond = cond.detach().cpu().permute(1, 2, 0)
    real = real.detach().cpu().permute(1, 2, 0)
    fake = fake.detach().cpu().permute(1, 2, 0)

    # Prepare images and titles
    images = [cond, real, fake]
    titles = ['Input', 'Real', 'Generated']

    print(f'Epoch: {current_epoch}')
    # Create a figure and axes
    fig, ax = plt.subplots(1, 3, figsize=figsize)

    for idx, img in enumerate(images):
        if idx == 0:  # Input condition image (L channel only)
            ab = torch.zeros((224, 224, 2))
            img = torch.cat([images[0] * 100, ab], dim=2).numpy()
            imgan = lab2rgb(img)
        else:  # Real or generated image (AB channels)
            imgan = lab_to_rgb(images[0], img)

        ax[idx].imshow(imgan)
        ax[idx].axis("off")
        ax[idx].set_title(titles[idx])

    # Save the figure as an image file
    output_path = os.path.join(output_dir, f"epoch__{current_epoch}.png")
    plt.savefig(output_path, bbox_inches="tight")
    plt.close(fig)

    print(f"Image saved to {output_path}")

class CWGAN(pl.LightningModule):
    def __init__(self, in_channels, out_channels, learning_rate=1e-4, lambda_recon=100, display_step=2, lambda_gp=10, lambda_r1=10):
        super().__init__()
        self.automatic_optimization = False
        self.save_hyperparameters()

        # Generator와 Critic 초기화
        self.generator = Generator(in_channels, out_channels)
        self.critic = Critic(in_channels + out_channels)

        # Optimizer 초기화
        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))
        self.optimizer_C = optim.Adam(self.critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))

        self.scheduler_G = StepLR(self.optimizer_G, step_size=10, gamma=0.1)
        self.scheduler_C = ReduceLROnPlateau(self.optimizer_C, mode='min', factor=0.5, patience=5)

        # 손실 함수 및 하이퍼파라미터
        self.lambda_recon = lambda_recon
        self.lambda_gp = lambda_gp
        self.lambda_r1 = lambda_r1
        self.recon_criterion = nn.L1Loss()
        self.generator_losses, self.critic_losses = [], []
        self.display_step = display_step

        # VGG 초기화 (Perceptual Loss)
        self.vgg = models.vgg16(pretrained=True).features[:16].eval()
        for param in self.vgg.parameters():
            param.requires_grad = False

    def configure_optimizers(self):
        return [self.optimizer_C, self.optimizer_G], [self.scheduler_C, self.scheduler_G]

    def perceptual_loss(self, fake_images, real_images):
        """Compute Perceptual Loss using VGG."""
        # 2채널 또는 1채널 이미지를 3채널로 변환
        if fake_images.size(1) == 1 or fake_images.size(1) == 2:
            fake_images = fake_images.repeat(1, 3, 1, 1)  # (B, C, H, W) -> (B, 3, H, W)
        if real_images.size(1) == 1 or real_images.size(1) == 2:
            real_images = real_images.repeat(1, 3, 1, 1)

        # 이미지 크기 리사이징 (224x224)
        fake_resized = F.interpolate(fake_images, size=(224, 224), mode="bilinear", align_corners=False)
        real_resized = F.interpolate(real_images, size=(224, 224), mode="bilinear", align_corners=False)

        # 6채널 데이터를 3채널로 변환
        if fake_resized.size(1) == 6:
            fake_resized = fake_resized[:, :3, :, :]  # 첫 3채널만 사용
        if real_resized.size(1) == 6:
            real_resized = real_resized[:, :3, :, :]  # 첫 3채널만 사용

        # print(fake_resized.shape)
        # print(real_resized.shape)

        # VGG 특성 추출
        fake_features = self.vgg(fake_resized)
        real_features = self.vgg(real_resized)

        # Perceptual Loss 계산
        return F.mse_loss(fake_features, real_features)

    def color_loss(self, fake_images, real_images):
        """Compute Color Loss in LAB color space."""
        # print(fake_images.shape) # torch.Size([1, 2, 224, 224])

        # 텐서의 차원을 (B, H, W, C) 순서로 변경
        fake_np = fake_images.detach().cpu().numpy().transpose(0, 2, 3, 1)
        real_np = real_images.detach().cpu().numpy().transpose(0, 2, 3, 1)

        L = np.full_like(real_np[..., 0], 50)  # 224, 224 크기
        L = np.expand_dims(L, axis=-1)
        # print(L.shape) # (1, 224, 224, 1)

        # rgb2lab에 넣기 위해 [0,1]사이로 정규화
        fake_np = np.clip(fake_np, 0, 1)

        # print(fake_np.shape) # (1, 224, 224, 2)

        # print(fake_np.min(), fake_np.max()) # 0.0 1.0

        fake_np_cat = np.concatenate([L, fake_np[..., 0:1], fake_np[..., 1:2]], axis=-1)  # (1, 224, 224, 3)
        # print(fake_np_cat.shape)
        fake_lab = lab2rgb(fake_np_cat)
        # print(fake_lab.shape)

        fake_ab = torch.tensor(fake_lab[..., 1:], dtype=torch.float32).to(fake_images.device)
        real_ab = torch.tensor(real_np[..., 1:], dtype=torch.float32).to(real_images.device)

        return F.l1_loss(fake_ab, real_ab)

    def generator_step(self, real_images, conditioned_images):
        self.optimizer_G.zero_grad()

        fake_images = self.generator(conditioned_images)

        # Losses
        recon_loss = self.recon_criterion(fake_images, real_images)
        perc_loss = self.perceptual_loss(fake_images, real_images)
        color_loss = self.color_loss(fake_images, real_images)

        # Total Generator Loss
        gen_loss = recon_loss + 0.005 * perc_loss + 0.01 * color_loss
        gen_loss.backward()
        self.optimizer_G.step()

        self.generator_losses.append(gen_loss.item())

    def critic_step(self, real_images, conditioned_images):
        self.optimizer_C.zero_grad()

        fake_images = self.generator(conditioned_images)
        fake_logits = self.critic(fake_images.detach(), conditioned_images)
        real_logits = self.critic(real_images, conditioned_images)

        # Wasserstein Loss
        loss_C = torch.abs(real_logits.mean() - fake_logits.mean())

        # Gradient Penalty
        alpha = torch.rand(real_images.size(0), 1, 1, 1, device=real_images.device)
        interpolated = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)
        interpolated_logits = self.critic(interpolated, conditioned_images)

        gradients = torch.autograd.grad(
            outputs=interpolated_logits,
            inputs=interpolated,
            grad_outputs=torch.ones_like(interpolated_logits),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]
        gradients = gradients.view(gradients.size(0), -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()

        loss_C += self.lambda_gp * gradient_penalty  # Gradient Penalty 추가
        loss_C.backward()
        self.optimizer_C.step()

        self.critic_losses.append(loss_C.item())


    def training_step(self, batch, batch_idx):
        real, condition = batch
        opt_critic, opt_generator = self.optimizers()

        # Critic update
        if batch_idx == 0:
            self.critic_step(real, condition)
            opt_critic.step()
            opt_critic.zero_grad()
            self.generator_step(real, condition)
            opt_generator.step()
            opt_generator.zero_grad()

        elif batch_idx % 5 != 0:  # Critic을 더 자주 업데이트
            self.critic_step(real, condition)
            opt_critic.step()
            opt_critic.zero_grad()

        # Generator update
        else:
            self.generator_step(real, condition)
            opt_generator.step()
            opt_generator.zero_grad()

        # Epoch마다 상태 저장
        # self.current_epoch % self.display_step == 0
        if batch_idx == 0:
            fake = self.generator(condition).detach()
            torch.save(self.generator.state_dict(), f"Generator_{self.current_epoch}.pt")
            torch.save(self.critic.state_dict(), f"Critic_{self.current_epoch}.pt")
            print(f"Epoch {self.current_epoch}: Generator loss: {sum(self.generator_losses) / len(self.generator_losses)}, Critic loss: {sum(self.critic_losses) / len(self.critic_losses)}")
            save_progress(condition[0], real[0], fake[0], self.current_epoch)

gc.collect()
cwgan = CWGAN(in_channels = 1, out_channels = 2, learning_rate=1e-4, lambda_recon=75, lambda_gp = 15, display_step=10)

trainer = pl.Trainer(max_epochs=150, accelerator='gpu', devices=-1)
trainer.fit(cwgan, train_loader)

