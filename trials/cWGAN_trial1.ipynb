{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOJxTMEeSCM16XSXIlKT70A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"fb5af36995a8467ebd31e8f8b397f1c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32653e14aa7f42f5ae45665d61a61ef9","IPY_MODEL_9a04ecfc7ccd42e4a1d3d51612a27a55","IPY_MODEL_38bf1ff8227047d6b59a915764e0253f"],"layout":"IPY_MODEL_0c4cd18039ee44dda0da7b1aad6cb143"}},"32653e14aa7f42f5ae45665d61a61ef9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18067fe59a9b49bab0b88004e80ac82a","placeholder":"​","style":"IPY_MODEL_9c2408c1a6704a59903f30269e47a495","value":"Epoch 40: 100%"}},"9a04ecfc7ccd42e4a1d3d51612a27a55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4c93d07bc624db9a3b011c0829bed42","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b526217b5ef4924ab6b40f3cb1d9da0","value":500}},"38bf1ff8227047d6b59a915764e0253f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23eb62227da9475bb60edd2e66c8b7d9","placeholder":"​","style":"IPY_MODEL_5c6926cd27b84b3680f69479ad5baa8a","value":" 500/500 [00:28&lt;00:00, 17.31it/s, v_num=2]"}},"0c4cd18039ee44dda0da7b1aad6cb143":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"18067fe59a9b49bab0b88004e80ac82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c2408c1a6704a59903f30269e47a495":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4c93d07bc624db9a3b011c0829bed42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b526217b5ef4924ab6b40f3cb1d9da0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23eb62227da9475bb60edd2e66c8b7d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c6926cd27b84b3680f69479ad5baa8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GmO2SGGJjWsY","executionInfo":{"status":"ok","timestamp":1732948901621,"user_tz":-540,"elapsed":88690,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"f5e8ac67-8338-4b53-8b79-a197210c7b81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/shravankumar9892/image-colorization?dataset_version_number=4...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2.10G/2.10G [00:37<00:00, 59.7MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/shravankumar9892/image-colorization/versions/4\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"shravankumar9892/image-colorization\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["!pip install torchsummary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voqwV-g97QTw","executionInfo":{"status":"ok","timestamp":1732950957387,"user_tz":-540,"elapsed":3921,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"13b6ad90-19ff-49e7-c353-319b9486bf08"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"]}]},{"cell_type":"code","source":["!pip install pytorch-lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFSCXY3h2_is","executionInfo":{"status":"ok","timestamp":1732949862040,"user_tz":-540,"elapsed":5666,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"d72107ed-7d72-4b8d-8554-d06571efd4c4"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.17.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n","Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"]}]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","\n","# Import glob to get the files directories recursively\n","import glob\n","\n","# Import Garbage collector interface\n","import gc\n","\n","# Import OpenCV to transforme pictures\n","import cv2\n","\n","# Import Time\n","import time\n","\n","# import numpy for math calculations\n","import numpy as np\n","\n","# Import pandas for data (csv) manipulation\n","import pandas as pd\n","\n","# Import matplotlib for plotting\n","import matplotlib.pyplot as plt\n","import matplotlib\n","matplotlib.style.use('fivethirtyeight')\n","%matplotlib inline\n","\n","import PIL\n","from PIL import Image\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import pytorch_lightning as pl\n","\n","# Import pytorch to build Deel Learling Models\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from torchvision import models\n","from torch.nn import functional as F\n","import torch.utils.data\n","from torchvision.models.inception import inception_v3\n","from scipy.stats import entropy\n","\n","from torchsummary import summary\n","\n","# Import tqdm to show a smart progress meter\n","from tqdm import tqdm\n","\n","# Import warnings to hide the unnessairy warniings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"xkYNsXCGzG5Y","executionInfo":{"status":"ok","timestamp":1732950960855,"user_tz":-540,"elapsed":272,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# path 변수에 저장된 디렉토리 내 파일 목록 출력\n","print(\"Dataset contents:\")\n","for root, dirs, files in os.walk(path):\n","    for file in files:\n","        print(os.path.join(root, file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8gOB5lFfjblH","executionInfo":{"status":"ok","timestamp":1732949931745,"user_tz":-540,"elapsed":321,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"a1a30b78-1c09-41ab-cfcf-2d21e5ebf120"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset contents:\n","/root/.cache/kagglehub/datasets/shravankumar9892/image-colorization/versions/4/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n","/root/.cache/kagglehub/datasets/shravankumar9892/image-colorization/versions/4/l/gray_scale.npy\n","/root/.cache/kagglehub/datasets/shravankumar9892/image-colorization/versions/4/ab/ab/ab2.npy\n","/root/.cache/kagglehub/datasets/shravankumar9892/image-colorization/versions/4/ab/ab/ab3.npy\n","/root/.cache/kagglehub/datasets/shravankumar9892/image-colorization/versions/4/ab/ab/ab1.npy\n"]}]},{"cell_type":"code","source":["# npy 파일 열기\n","gray_scale_path = f\"{path}/l/gray_scale.npy\"\n","ab1_path = f\"{path}/ab/ab/ab1.npy\"\n","\n","# 데이터를 로드\n","L_df = (np.load(gray_scale_path))[:500]\n","ab1_df = (np.load(ab1_path))[:500]\n","\n","# 데이터 확인\n","print(\"Gray Scale Shape:\", L_df.shape) # (h, w) 형식\n","print(\"AB1 Shape:\", ab1_df.shape) # (h, w, 2) 형식"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1B4Li2Rj6eK","executionInfo":{"status":"ok","timestamp":1732950456577,"user_tz":-540,"elapsed":2385,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"6d1b906d-a06b-4c42-9eb3-dfd3e778710f"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Gray Scale Shape: (500, 224, 224)\n","AB1 Shape: (500, 224, 224, 2)\n"]}]},{"cell_type":"code","source":["print(\"AB1 min:\", ab1_df.min(), \"max:\", ab1_df.max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0tOnDMc2EBt","executionInfo":{"status":"ok","timestamp":1732950458391,"user_tz":-540,"elapsed":384,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"b3a18604-0831-419d-ac08-918ebe29b341"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["AB1 min: 20 max: 223\n"]}]},{"cell_type":"code","source":["L_df.min(), L_df.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uH90W3KHw6JN","executionInfo":{"status":"ok","timestamp":1732950459929,"user_tz":-540,"elapsed":388,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"1fda81b7-534d-4f5e-d953-85bea93d69c8"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 255)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["dataset = (L_df, ab1_df)\n","gc.collect()"],"metadata":{"id":"aJ0FswHgSKcr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732950461973,"user_tz":-540,"elapsed":312,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"59bae6b5-a1fb-4683-9527-e66f4527bb8d"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["59546"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["def lab_to_rgb(L, ab):\n","    \"\"\"\n","    Takes an image or a batch of images and converts from LAB space to RGB\n","    \"\"\"\n","    L = L  * 100\n","    ab = (ab - 0.5) * 128 * 2\n","    Lab = torch.cat([L, ab], dim=2).numpy()\n","    rgb_imgs = []\n","    for img in Lab:\n","        img_rgb = lab2rgb(img)\n","        rgb_imgs.append(img_rgb)\n","    return np.stack(rgb_imgs, axis=0)"],"metadata":{"id":"bPrb9SBR1_i_","executionInfo":{"status":"ok","timestamp":1732950463396,"user_tz":-540,"elapsed":280,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(30,30))\n","for i in range(1,16,2):\n","    plt.subplot(4,4,i)\n","    img = np.zeros((224,224,3))\n","    img[:,:,0] = L_df[i]\n","    plt.title('B&W')\n","    plt.imshow(lab2rgb(img))\n","\n","    plt.subplot(4,4,i+1)\n","    img[:,:,1:] = ab1_df[i]\n","    img = img.astype('uint8')\n","    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n","    plt.title('Colored')\n","    plt.imshow(img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712,"output_embedded_package_id":"1h71rdt3hDTeEPujB9OBca1vJxTd6mfXO"},"id":"urun5sMH1_lD","executionInfo":{"status":"ok","timestamp":1732950477330,"user_tz":-540,"elapsed":12073,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"d363d6c6-0a9e-4978-a49e-b3d7e8829072"},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TzucX0x1_nZ","executionInfo":{"status":"ok","timestamp":1732950072472,"user_tz":-540,"elapsed":343,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"17e13473-a803-4408-b36c-9f743c7c7447"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["59779"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":[],"metadata":{"id":"5iaw1hvL1_p_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Loader"],"metadata":{"id":"jjqkjtEr4vSG"}},{"cell_type":"code","source":["class ImageColorizationDataset(Dataset):\n","    ''' Black and White (L) Images and corresponding A&B Colors'''\n","    def __init__(self, dataset, transform=None):\n","        '''\n","        :param dataset: Dataset name.\n","        :param data_dir: Directory with all the images.\n","        :param transform: Optional transform to be applied on sample\n","        '''\n","        self.dataset = dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset[0])\n","\n","    def __getitem__(self, idx):\n","        L = np.array(dataset[0][idx]).reshape((224,224,1))\n","        L = transforms.ToTensor()(L)\n","\n","        ab = np.array(dataset[1][idx])\n","        ab = transforms.ToTensor()(ab)\n","\n","        return ab, L"],"metadata":{"id":"sB64EAHS4xRG","executionInfo":{"status":"ok","timestamp":1732950553365,"user_tz":-540,"elapsed":307,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["batch_size = 1\n","\n","# Prepare the Datasets\n","train_dataset = ImageColorizationDataset(dataset = (L_df, ab1_df))\n","test_dataset = ImageColorizationDataset(dataset = (L_df, ab1_df))\n","\n","# Build DataLoaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True, pin_memory = True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = False, pin_memory = True)"],"metadata":{"id":"-2UjYuyY41qA","executionInfo":{"status":"ok","timestamp":1732950562446,"user_tz":-540,"elapsed":332,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["class ResBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, stride=stride, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels,kernel_size=3,padding=1, stride=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.identity_map = nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride)\n","        self.relu = nn.ReLU(inplace=True)\n","    def forward(self, inputs):\n","        x = inputs.clone().detach()\n","        out = self.layer(x)\n","        residual  = self.identity_map(inputs)\n","        skip = out + residual\n","        return self.relu(skip)"],"metadata":{"id":"mc5pHnFm5xOC","executionInfo":{"status":"ok","timestamp":1732950581666,"user_tz":-540,"elapsed":316,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["class DownSampleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            ResBlock(in_channels, out_channels)\n","        )\n","\n","    def forward(self, inputs):\n","        return self.layer(inputs)"],"metadata":{"id":"6sL4yWs3516_","executionInfo":{"status":"ok","timestamp":1732950582781,"user_tz":-540,"elapsed":4,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["class UpSampleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n","\n","    def forward(self, inputs, skip):\n","        x = self.upsample(inputs)\n","        x = torch.cat([x, skip], dim=1)\n","        x = self.res_block(x)\n","        return x"],"metadata":{"id":"1JNc36wU52JD","executionInfo":{"status":"ok","timestamp":1732950587661,"user_tz":-540,"elapsed":509,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self, input_channel, output_channel, dropout_rate = 0.2):\n","        super().__init__()\n","        self.encoding_layer1_ = ResBlock(input_channel,64)\n","        self.encoding_layer2_ = DownSampleConv(64, 128)\n","        self.encoding_layer3_ = DownSampleConv(128, 256)\n","        self.bridge = DownSampleConv(256, 512)\n","        self.decoding_layer3_ = UpSampleConv(512, 256)\n","        self.decoding_layer2_ = UpSampleConv(256, 128)\n","        self.decoding_layer1_ = UpSampleConv(128, 64)\n","        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n","        self.dropout = nn.Dropout2d(dropout_rate)\n","\n","    def forward(self, inputs):\n","        ###################### Enocoder #########################\n","        e1 = self.encoding_layer1_(inputs)\n","        e1 = self.dropout(e1)\n","        e2 = self.encoding_layer2_(e1)\n","        e2 = self.dropout(e2)\n","        e3 = self.encoding_layer3_(e2)\n","        e3 = self.dropout(e3)\n","\n","        ###################### Bridge #########################\n","        bridge = self.bridge(e3)\n","        bridge = self.dropout(bridge)\n","\n","        ###################### Decoder #########################\n","        d3 = self.decoding_layer3_(bridge, e3)\n","        d2 = self.decoding_layer2_(d3, e2)\n","        d1 = self.decoding_layer1_(d2, e1)\n","\n","        ###################### Output #########################\n","        output = self.output(d1)\n","        return output"],"metadata":{"id":"ZEqSB21k53Vn","executionInfo":{"status":"ok","timestamp":1732950616931,"user_tz":-540,"elapsed":291,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["model = Generator(1,2).to(device)\n","summary(model, (1, 224, 224), batch_size = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hnW1NBW-5-h3","executionInfo":{"status":"ok","timestamp":1732950624183,"user_tz":-540,"elapsed":2469,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"5dd38559-47ca-4976-9f7b-49f514302a6b"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [1, 64, 224, 224]             576\n","       BatchNorm2d-2          [1, 64, 224, 224]             128\n","              ReLU-3          [1, 64, 224, 224]               0\n","            Conv2d-4          [1, 64, 224, 224]          36,864\n","       BatchNorm2d-5          [1, 64, 224, 224]             128\n","              ReLU-6          [1, 64, 224, 224]               0\n","            Conv2d-7          [1, 64, 224, 224]             128\n","              ReLU-8          [1, 64, 224, 224]               0\n","          ResBlock-9          [1, 64, 224, 224]               0\n","        Dropout2d-10          [1, 64, 224, 224]               0\n","        MaxPool2d-11          [1, 64, 112, 112]               0\n","           Conv2d-12         [1, 128, 112, 112]          73,728\n","      BatchNorm2d-13         [1, 128, 112, 112]             256\n","             ReLU-14         [1, 128, 112, 112]               0\n","           Conv2d-15         [1, 128, 112, 112]         147,456\n","      BatchNorm2d-16         [1, 128, 112, 112]             256\n","             ReLU-17         [1, 128, 112, 112]               0\n","           Conv2d-18         [1, 128, 112, 112]           8,320\n","             ReLU-19         [1, 128, 112, 112]               0\n","         ResBlock-20         [1, 128, 112, 112]               0\n","   DownSampleConv-21         [1, 128, 112, 112]               0\n","        Dropout2d-22         [1, 128, 112, 112]               0\n","        MaxPool2d-23           [1, 128, 56, 56]               0\n","           Conv2d-24           [1, 256, 56, 56]         294,912\n","      BatchNorm2d-25           [1, 256, 56, 56]             512\n","             ReLU-26           [1, 256, 56, 56]               0\n","           Conv2d-27           [1, 256, 56, 56]         589,824\n","      BatchNorm2d-28           [1, 256, 56, 56]             512\n","             ReLU-29           [1, 256, 56, 56]               0\n","           Conv2d-30           [1, 256, 56, 56]          33,024\n","             ReLU-31           [1, 256, 56, 56]               0\n","         ResBlock-32           [1, 256, 56, 56]               0\n","   DownSampleConv-33           [1, 256, 56, 56]               0\n","        Dropout2d-34           [1, 256, 56, 56]               0\n","        MaxPool2d-35           [1, 256, 28, 28]               0\n","           Conv2d-36           [1, 512, 28, 28]       1,179,648\n","      BatchNorm2d-37           [1, 512, 28, 28]           1,024\n","             ReLU-38           [1, 512, 28, 28]               0\n","           Conv2d-39           [1, 512, 28, 28]       2,359,296\n","      BatchNorm2d-40           [1, 512, 28, 28]           1,024\n","             ReLU-41           [1, 512, 28, 28]               0\n","           Conv2d-42           [1, 512, 28, 28]         131,584\n","             ReLU-43           [1, 512, 28, 28]               0\n","         ResBlock-44           [1, 512, 28, 28]               0\n","   DownSampleConv-45           [1, 512, 28, 28]               0\n","        Dropout2d-46           [1, 512, 28, 28]               0\n","         Upsample-47           [1, 512, 56, 56]               0\n","           Conv2d-48           [1, 256, 56, 56]       1,769,472\n","      BatchNorm2d-49           [1, 256, 56, 56]             512\n","             ReLU-50           [1, 256, 56, 56]               0\n","           Conv2d-51           [1, 256, 56, 56]         589,824\n","      BatchNorm2d-52           [1, 256, 56, 56]             512\n","             ReLU-53           [1, 256, 56, 56]               0\n","           Conv2d-54           [1, 256, 56, 56]         196,864\n","             ReLU-55           [1, 256, 56, 56]               0\n","         ResBlock-56           [1, 256, 56, 56]               0\n","     UpSampleConv-57           [1, 256, 56, 56]               0\n","         Upsample-58         [1, 256, 112, 112]               0\n","           Conv2d-59         [1, 128, 112, 112]         442,368\n","      BatchNorm2d-60         [1, 128, 112, 112]             256\n","             ReLU-61         [1, 128, 112, 112]               0\n","           Conv2d-62         [1, 128, 112, 112]         147,456\n","      BatchNorm2d-63         [1, 128, 112, 112]             256\n","             ReLU-64         [1, 128, 112, 112]               0\n","           Conv2d-65         [1, 128, 112, 112]          49,280\n","             ReLU-66         [1, 128, 112, 112]               0\n","         ResBlock-67         [1, 128, 112, 112]               0\n","     UpSampleConv-68         [1, 128, 112, 112]               0\n","         Upsample-69         [1, 128, 224, 224]               0\n","           Conv2d-70          [1, 64, 224, 224]         110,592\n","      BatchNorm2d-71          [1, 64, 224, 224]             128\n","             ReLU-72          [1, 64, 224, 224]               0\n","           Conv2d-73          [1, 64, 224, 224]          36,864\n","      BatchNorm2d-74          [1, 64, 224, 224]             128\n","             ReLU-75          [1, 64, 224, 224]               0\n","           Conv2d-76          [1, 64, 224, 224]          12,352\n","             ReLU-77          [1, 64, 224, 224]               0\n","         ResBlock-78          [1, 64, 224, 224]               0\n","     UpSampleConv-79          [1, 64, 224, 224]               0\n","           Conv2d-80           [1, 2, 224, 224]             130\n","================================================================\n","Total params: 8,216,194\n","Trainable params: 8,216,194\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.19\n","Forward/backward pass size (MB): 1006.80\n","Params size (MB): 31.34\n","Estimated Total Size (MB): 1038.33\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["class Critic(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super(Critic, self).__init__()\n","\n","        def critic_block(in_filters, out_filters, normalization=True):\n","            \"\"\"Returns layers of each critic block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n","            if normalization:\n","                layers.append(nn.InstanceNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *critic_block(in_channels, 64, normalization=False),\n","            *critic_block(64, 128),\n","            *critic_block(128, 256),\n","            *critic_block(256, 512),\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Flatten(),\n","            nn.Linear(512, 1)\n","        )\n","\n","    def forward(self, ab, l):\n","        # Concatenate image and condition image by channels to produce input\n","        img_input = torch.cat((ab, l), 1)\n","        output = self.model(img_input)\n","        return output"],"metadata":{"id":"prdWoVS35_xO","executionInfo":{"status":"ok","timestamp":1732950640935,"user_tz":-540,"elapsed":376,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["model = Critic(3).to(device)\n","summary(model, [(2, 224, 224), (1, 224, 224)], batch_size = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxVHImhu6EXU","executionInfo":{"status":"ok","timestamp":1732950642719,"user_tz":-540,"elapsed":317,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"8529b481-734f-4845-a4ae-ebbd20b0712d"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [1, 64, 112, 112]           3,136\n","         LeakyReLU-2          [1, 64, 112, 112]               0\n","            Conv2d-3           [1, 128, 56, 56]         131,200\n","    InstanceNorm2d-4           [1, 128, 56, 56]               0\n","         LeakyReLU-5           [1, 128, 56, 56]               0\n","            Conv2d-6           [1, 256, 28, 28]         524,544\n","    InstanceNorm2d-7           [1, 256, 28, 28]               0\n","         LeakyReLU-8           [1, 256, 28, 28]               0\n","            Conv2d-9           [1, 512, 14, 14]       2,097,664\n","   InstanceNorm2d-10           [1, 512, 14, 14]               0\n","        LeakyReLU-11           [1, 512, 14, 14]               0\n","AdaptiveAvgPool2d-12             [1, 512, 1, 1]               0\n","          Flatten-13                   [1, 512]               0\n","           Linear-14                     [1, 1]             513\n","================================================================\n","Total params: 2,757,057\n","Trainable params: 2,757,057\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 19208.00\n","Forward/backward pass size (MB): 28.34\n","Params size (MB): 10.52\n","Estimated Total Size (MB): 19246.85\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n","def _weights_init(m):\n","    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)"],"metadata":{"id":"8yK1Uj1b6E0B","executionInfo":{"status":"ok","timestamp":1732950659851,"user_tz":-540,"elapsed":284,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["def display_progress(cond, real, fake, current_epoch = 0, figsize=(20,15)):\n","    \"\"\"\n","    Save cond, real (original) and generated (fake)\n","    images in one panel\n","    \"\"\"\n","    cond = cond.detach().cpu().permute(1, 2, 0)\n","    real = real.detach().cpu().permute(1, 2, 0)\n","    fake = fake.detach().cpu().permute(1, 2, 0)\n","\n","    images = [cond, real, fake]\n","    titles = ['input','real','generated']\n","    print(f'Epoch: {current_epoch}')\n","    fig, ax = plt.subplots(1, 3, figsize=figsize)\n","    for idx,img in enumerate(images):\n","        if idx == 0:\n","            ab = torch.zeros((224,224,2))\n","            img = torch.cat([images[0]* 100, ab], dim=2).numpy()\n","            imgan = lab2rgb(img)\n","        else:\n","            imgan = lab_to_rgb(images[0],img)\n","        ax[idx].imshow(imgan)\n","        ax[idx].axis(\"off\")\n","    for idx, title in enumerate(titles):\n","        ax[idx].set_title('{}'.format(title))\n","    plt.show()"],"metadata":{"id":"cmu3VRpJ6JBA","executionInfo":{"status":"ok","timestamp":1732950671891,"user_tz":-540,"elapsed":470,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["class CWGAN(pl.LightningModule):\n","\n","    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=100, display_step=10, lambda_gp=10, lambda_r1=10,):\n","\n","        super().__init__()\n","        self.automatic_optimization = False\n","\n","        self.save_hyperparameters()\n","\n","        self.display_step = display_step\n","\n","        self.generator = Generator(in_channels, out_channels)\n","        self.critic = Critic(in_channels + out_channels)\n","        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n","        self.optimizer_C = optim.Adam(self.critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n","        self.lambda_recon = lambda_recon\n","        self.lambda_gp = lambda_gp\n","        self.lambda_r1 = lambda_r1\n","        self.recon_criterion = nn.L1Loss()\n","        self.generator_losses, self.critic_losses  =[],[]\n","\n","    def configure_optimizers(self):\n","        return [self.optimizer_C, self.optimizer_G]\n","\n","    def generator_step(self, real_images, conditioned_images):\n","        # WGAN has only a reconstruction loss\n","        self.optimizer_G.zero_grad()\n","        fake_images = self.generator(conditioned_images)\n","        recon_loss = self.recon_criterion(fake_images, real_images)\n","        recon_loss.backward()\n","        self.optimizer_G.step()\n","\n","        # Keep track of the average generator loss\n","        self.generator_losses += [recon_loss.item()]\n","\n","    def critic_step(self, real_images, conditioned_images):\n","        self.optimizer_C.zero_grad()\n","        fake_images = self.generator(conditioned_images)\n","        fake_logits = self.critic(fake_images, conditioned_images)\n","        real_logits = self.critic(real_images, conditioned_images)\n","\n","        # Compute the loss for the critic\n","        loss_C = real_logits.mean() - fake_logits.mean()\n","\n","        # Compute the gradient penalty\n","        alpha = torch.rand(real_images.size(0), 1, 1, 1, requires_grad=True)\n","        alpha = alpha.to(device)\n","        interpolated = (alpha * real_images + (1 - alpha) * fake_images.detach()).requires_grad_(True)\n","\n","        interpolated_logits = self.critic(interpolated, conditioned_images)\n","\n","        grad_outputs = torch.ones_like(interpolated_logits, dtype=torch.float32, requires_grad=True)\n","        gradients = torch.autograd.grad(outputs=interpolated_logits, inputs=interpolated, grad_outputs=grad_outputs,create_graph=True, retain_graph=True)[0]\n","\n","\n","        gradients = gradients.view(len(gradients), -1)\n","        gradients_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","        loss_C += self.lambda_gp * gradients_penalty\n","\n","        # Compute the R1 regularization loss\n","        r1_reg = gradients.pow(2).sum(1).mean()\n","        loss_C += self.lambda_r1 * r1_reg\n","\n","        # Backpropagation\n","        loss_C.backward()\n","        self.optimizer_C.step()\n","        self.critic_losses += [loss_C.item()]\n","\n","    def training_step(self, batch, batch_idx):\n","      real, condition = batch\n","      # 두 Optimizer 가져오기\n","      opt_critic, opt_generator = self.optimizers()\n","\n","      # Critic 업데이트 단계\n","      if batch_idx % 2 == 0:  # Critic과 Generator를 번갈아 업데이트\n","          self.critic_step(real, condition)\n","          opt_critic.step()\n","          opt_critic.zero_grad()\n","\n","      # Generator 업데이트 단계\n","      else:\n","          self.generator_step(real, condition)\n","          opt_generator.step()\n","          opt_generator.zero_grad()\n","\n","      # Loss 계산 및 기록\n","      gen_mean = sum(self.generator_losses[-self.display_step:]) / self.display_step\n","      crit_mean = sum(self.critic_losses[-self.display_step:]) / self.display_step\n","\n","      # Epoch마다 상태 저장 및 출력\n","      if self.current_epoch % self.display_step == 0 and batch_idx == 0:\n","          fake = self.generator(condition).detach()\n","          torch.save(self.generator.state_dict(), f\"ResUnet_{self.current_epoch}.pt\")\n","          torch.save(self.critic.state_dict(), f\"PatchGAN_{self.current_epoch}.pt\")\n","          print(f\"Epoch {self.current_epoch} : Generator loss: {gen_mean}, Critic loss: {crit_mean}\")\n","          display_progress(condition[0], real[0], fake[0], self.current_epoch)\n"],"metadata":{"id":"ui1rP2Uh6L5x","executionInfo":{"status":"ok","timestamp":1732951243320,"user_tz":-540,"elapsed":305,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["gc.collect()\n","cwgan = CWGAN(in_channels = 1, out_channels = 2 ,learning_rate=2e-4, lambda_recon=100, display_step=10)"],"metadata":{"id":"7lZ1R5Ov6lrq","executionInfo":{"status":"ok","timestamp":1732951245809,"user_tz":-540,"elapsed":1043,"user":{"displayName":"최진영","userId":"08127513691066717192"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["trainer = pl.Trainer(max_epochs=150, accelerator='gpu', devices=-1)\n","trainer.fit(cwgan, train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fb5af36995a8467ebd31e8f8b397f1c8","32653e14aa7f42f5ae45665d61a61ef9","9a04ecfc7ccd42e4a1d3d51612a27a55","38bf1ff8227047d6b59a915764e0253f","0c4cd18039ee44dda0da7b1aad6cb143","18067fe59a9b49bab0b88004e80ac82a","9c2408c1a6704a59903f30269e47a495","b4c93d07bc624db9a3b011c0829bed42","2b526217b5ef4924ab6b40f3cb1d9da0","23eb62227da9475bb60edd2e66c8b7d9","5c6926cd27b84b3680f69479ad5baa8a"],"output_embedded_package_id":"1smie_KE1m7fjsILC5caiPfdeap5zsM8q"},"id":"Nq_xX0hD6nIU","executionInfo":{"status":"error","timestamp":1732952540712,"user_tz":-540,"elapsed":1294914,"user":{"displayName":"최진영","userId":"08127513691066717192"}},"outputId":"63b52b90-3d14-470e-e420-ea861e4115c0"},"execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"svBBhvYO6obT"},"execution_count":null,"outputs":[]}]}